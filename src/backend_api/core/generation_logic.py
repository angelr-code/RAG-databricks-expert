import opik

import json

from collections.abc import AsyncGenerator

from src.backend_api.models.api_models import SearchResult, QueryRequest, QueryResponse
from src.backend_api.models.provider_models import ModelConfig
from src.utils.logger import setup_logging
from src.backend_api.core.utils.openai_provider import generate_openai, stream_openai
from src.backend_api.core.utils.openrouter_provider import generate_openrouter, stream_openrouter

logger = setup_logging()

PROMPT = """
You are a skilled technical assistant specialized in Databricks.
Respond to the userâ€™s query using the provided context from these articles and pieces of documentation,
which are retrieved from a vector database, without relying on outside knowledge or assumptions.


### CRITICAL FORMATTING REQUIREMENTS:

1. **Use Clean Markdown Structure:**
   - Use `##` for main sections (e.g., "## What is Delta Sharing?")
   - Use `###` for subsections (e.g., "### Key Features")
   - Always add blank lines before and after headers
   - Use bullet points (`*`) for lists
   - Use `**bold**` sparingly for emphasis only

2. **Response Structure:**
   - Start with a brief 1-2 sentence overview
   - Organize information into clear sections with headers
   - Use short paragraphs (2-4 sentences maximum)
   - Use bullet points for features, steps, or lists
   - End with relevant sources/references if appropriate

3. **Writing Style:**
   - Be concise and technical but accessible
   - Use active voice
   - Avoid redundancy
   - No need to mention "based on the provided context"
   - If information is missing, state it clearly and suggest alternatives

4. **Token Limit:**
   - Maximum **{tokens} tokens** for your response
   - Prioritize clarity over length

5. **FORBIDDEN:**
   - Do NOT use HTML tags or colored text
   - Do NOT output plain text without markdown formatting
   - Do NOT create wall-of-text paragraphs
   - Do NOT use citation numbers like [1], [2], [3]
   - Do NOT fabricate information or make assumptions beyond the provided context. If the answer is not in the context, state that it is not available and suggest 
        alternatives or where to find more information such as technical blogs specialized in building Databricks solutions.
   - Do NOT cite sources in your answer, these will be provided separately in a context section

   ### EXAMPLE OF GOOD FORMATTING:

## What is Delta Sharing?

Delta Sharing is a secure, open protocol for data sharing developed by Databricks. It enables organizations to share data assets with external parties regardless of their platform.

### Key Features

* **Open Protocol**: Works across different computing platforms
* **Secure Access**: Built-in authentication and authorization
* **Multiple Assets**: Supports tables, notebooks, and AI models

### How It Works

Delta Sharing operates as an open protocol that allows data providers to expose shared assets via a catalog. Recipients can discover and access these assets securely using tokens generated by the provider's system.


### Query:
{query}

### Context Articles:
{context_texts}

### Final Answer:
"""

def build_prompt(query_text: str, contexts: SearchResult, max_tokens: int) -> str:
    """
    Build the prompt for the LLM using the query text and retrieved contexts.
    Args:
        query_text (str): The user's query text.
        contexts (SearchResult): The retrieved contexts from the vector database.
        max_tokens (int): The maximum number of tokens for the response.
        
    Returns:
        str: The formatted prompt string.
    """
    contexts_list = [
        f"- URL: {source}\n Content: {context}" for source, context in zip(contexts.sources, contexts.contexts)
    ]
    contexts = "\n\n".join(contexts_list)

    return PROMPT.format(
        query = query_text,
        context_texts=contexts,
        tokens=max_tokens
    )

@opik.track(name="generate_answer")
async def generate_answer(query_request: QueryRequest, search_result: SearchResult, user_api_key: str | None = None) -> QueryResponse:
    """
    Generate an answer based on the query request and search results.
    
    Args:
        query_request (QueryRequest): The user's query request.
        search_result (SearchResult): The search results from the vector database.
        user_api_key (str | None): The user's API key for the LLM provider.
    
    Returns:
        QueryResponse: The generated response including the answer and metadata.
    """

    selected_model = query_request.model or "gpt-4o-mini"
    config = ModelConfig(requested_model=selected_model)

    prompt = build_prompt(query_request.query_text, search_result, config.max_completion_tokens)
    
    if query_request.provider == "openai":
        logger.info("OpenAI provider selected for generation...")
        answer_text, model_used = await generate_openai(prompt, config, user_api_key)
    elif query_request.provider == "OpenRouter":
        logger.info("OpenRouter provider selected for generation...")
        answer_text, model_used = await generate_openrouter(prompt, config)     
    else:
        logger.warning(f"Unsupported provider: {query_request.provider}. Selecting ")

    response = QueryResponse(
        query_text=query_request.query_text,
        provider=query_request.provider,
        model=model_used,
        answer=answer_text,
        sources=list(search_result.sources)
    )

    return response

async def generate_streaming_answer(query_request: QueryRequest, search_result: SearchResult, user_api_key: str | None = None) -> AsyncGenerator[str,None]:
    """
    Generate a streaming answer based on the query request and search results.

    Args:
        query_request (QueryRequest): The user's query request.
        search_result (SearchResult): The search results from the vector database.
        user_api_key (str | None): The user's API key for the LLM provider.
    
    Yields:
        AsyncGenerator[str, None]: An asynchronous generator yielding response chunks.
    """
    selected_model = query_request.model or "gpt-4o-mini"
    config = ModelConfig(requested_model=selected_model)

    prompt = build_prompt(query_request.query_text, search_result, config.max_completion_tokens)

    sources_data = json.dumps({
        "type": "sources", 
        "data": list(search_result.sources)
    })
    yield f"{sources_data}\n"

    generator = None
    if query_request.provider == "openai":
        logger.info("OpenAI provider selected for generation...")
        generator = stream_openai(prompt, config, user_api_key)
    elif query_request.provider == "OpenRouter":
        logger.info("OpenRouter provider selected for generation...")
        generator = stream_openrouter(prompt, config)   
    else:
        logger.warning(f"Unsupported provider: {query_request.provider}. Selecting ")
        return 
    
    async for chunk in generator:
        yield chunk