# This Dockerfile is used to build a container image for deploying the FastAPI backend API
# to AWS Lambda using the AWS Lambda web adapter for FastAPI with streaming response support.
# Reference regarding AWS Lambda web adapter for FastAPI streaming configuration: 
#    https://github.com/awslabs/aws-lambda-web-adapter/blob/main/examples/fastapi-response-streaming/README.md 

FROM python:3.11-slim

# Working directory inside the container
WORKDIR /var/task
ENV PYTHONPATH=/var/task

# Lambda web adapter to allow FastAPI to run in AWS Lambda environment with streaming response support
COPY --from=public.ecr.aws/awsguru/aws-lambda-adapter:0.9.1 /lambda-adapter /opt/extensions/lambda-adapter

# Needed configuration for the adapter
ENV PORT=8000
ENV AWS_LWA_INVOKE_MODE=response_stream

# Basic dependencies installation for slim images and install Python dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*
COPY src/backend_api/requirements.txt .
RUN pip install -r requirements.txt --target ${PYTHONPATH} --no-cache-dir

# Pre-download FastEmbed models and store them in a fastembed cache directory in the Python path.
# This ensures that the models are available at runtime without needing to download them on-the-fly, with lambda possibly crashing.
ENV FASTEMBED_CACHE_PATH="${PYTHONPATH}/fastembed_cache"
RUN mkdir -p ${FASTEMBED_CACHE_PATH}
RUN python3 -c "from fastembed import TextEmbedding, SparseTextEmbedding; \
    print('Downloading dense model...'); \
    TextEmbedding(model_name='BAAI/bge-small-en-v1.5', cache_dir='${FASTEMBED_CACHE_PATH}'); \
    print('Downloading sparse model...'); \
    SparseTextEmbedding(model_name='Qdrant/bm25', cache_dir='${FASTEMBED_CACHE_PATH}');"

# We copy the src code to the 'src' directory inside the Python path in order to 
# keep the module structure and allow for proper imports from utils.logger and db.qdrant.qdrant_client.
COPY src/ ${PYTHONPATH}/src


CMD ["python3", "-m", "uvicorn", "src.backend_api.main:app", "--host", "0.0.0.0", "--port", "8000"]


####  IGNORE BELOW THIS LINE - FOR REFERENCE ONLY #######

# Later version using AWS Lambda base image (changed due to streaming response issues. Response streaming was sent as a whole after completion instead of chunked)
# Note:Â For using this version, a Mangum handler is needed in main.py

# FROM public.ecr.aws/docker/library/python:3.12.0-slim-bullseye
# COPY --from=public.ecr.aws/awsguru/aws-lambda-adapter:0.9.1 /lambda-adapter /opt/extensions/lambda-adapter

# # Copy the requirements file to the Lambda task root directory and install the dependencies.
# COPY src/backend_api/requirements.txt ${LAMBDA_TASK_ROOT}
# RUN pip install --upgrade pip && \
#     pip install -r requirements.txt --target ${LAMBDA_TASK_ROOT} --no-cache-dir

# # Pre-download FastEmbed models and store them in the Lambda task root directory.
# # This ensures that the models are available at runtime without needing to download them on-the-fly, with lambda possibly crashing.
# ENV FASTEMBED_CACHE_PATH="${LAMBDA_TASK_ROOT}/fastembed_cache"

# RUN mkdir -p ${FASTEMBED_CACHE_PATH}
# RUN PYTHONPATH="${LAMBDA_TASK_ROOT}" python3 -c "from fastembed import TextEmbedding, SparseTextEmbedding; \
#     print('Downloading dense model...'); \
#     TextEmbedding(model_name='BAAI/bge-small-en-v1.5', cache_dir='${FASTEMBED_CACHE_PATH}'); \
#     print('Downloading sparse model...'); \
#     SparseTextEmbedding(model_name='Qdrant/bm25', cache_dir='${FASTEMBED_CACHE_PATH}');"

# # We copy the src code to the 'src' directory inside the Lambda task root in order to 
# # keep the module structure and allow for proper imports from utils.logger and db.qdrant.qdrant_client.
# COPY src/ ${LAMBDA_TASK_ROOT}/src

# # Set an environment variable for FastEmbed cache path
# ENV FASTEMBED_CACHE_PATH="${LAMBDA_TASK_ROOT}/fastembed_cache"

# # Set the CMD command to the handler function to execute when a request is made to the Lambda function.
# CMD [ "src.backend_api.main.handler" ]
